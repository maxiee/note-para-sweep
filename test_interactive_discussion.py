#!/usr/bin/env python3
"""
äº¤äº’å¼å»ºè®®è®¨è®ºåŠŸèƒ½æµ‹è¯•è„šæœ¬
æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨æ–°çš„å¯¹è¯åŠŸèƒ½
"""

from pathlib import Path
import sys
import json

# æ·»åŠ srcç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent / "src"))

from note_para_sweep.config import Config
from note_para_sweep.llm_client import LLMClient
from note_para_sweep.file_operations import FileOperator


def test_interactive_discussion():
    """æµ‹è¯•äº¤äº’å¼è®¨è®ºåŠŸèƒ½"""
    print("ğŸ§ª æµ‹è¯•äº¤äº’å¼å»ºè®®è®¨è®ºåŠŸèƒ½\n")

    # åˆå§‹åŒ–é…ç½®ï¼ˆä½¿ç”¨mockæ¨¡å¼ï¼‰
    config = Config("config.yaml")
    llm_client = LLMClient(config)
    file_operator = FileOperator(dry_run=True)

    # æ¨¡æ‹Ÿä¸€ä¸ªå»ºè®®
    mock_suggestion = {
        "type": "rename",
        "priority": "high",
        "description": "å°†'ç§»åŠ¨åº”ç”¨å¼€å‘'é‡å‘½åä¸º'App v1.0 2024-Q3'",
        "current_path": "1. Projects/ç§»åŠ¨åº”ç”¨å¼€å‘",
        "suggested_path": "1. Projects/App v1.0 2024-Q3",
        "reasoning": "é¡¹ç›®å¿…é¡»å¯å®Œæˆä¸”å¯è¡¡é‡ï¼Œæ¨¡ç³Šåç§°ä¼šæ‹–å»¶é—­ç¯ã€‚",
    }

    print("ğŸ“‹ æ¨¡æ‹Ÿå»ºè®®:")
    print(json.dumps(mock_suggestion, ensure_ascii=False, indent=2))
    print()

    # å¼€å§‹å¯¹è¯
    print("ğŸ¤– å¼€å§‹å¯¹è¯...")
    llm_client.start_suggestion_conversation(mock_suggestion)

    # æ¨¡æ‹Ÿç”¨æˆ·åé¦ˆ
    user_feedbacks = [
        "è¿™ä¸ªé¡¹ç›®å®é™…ä¸Šå«'æ™ºèƒ½è®°è´¦App'ï¼Œè®¡åˆ’åœ¨2025å¹´2æœˆå‘å¸ƒ",
        "å¾ˆå¥½ï¼Œä½†æ˜¯æˆ‘è§‰å¾—ä¸éœ€è¦åŠ æœˆä»½ï¼Œåªå†™å¹´ä»½å°±å¤Ÿäº†",
        "æ»¡æ„",
    ]

    for i, feedback in enumerate(user_feedbacks, 1):
        print(f"[{i}] ç”¨æˆ·: {feedback}")

        if feedback == "æ»¡æ„":
            break

        result = llm_client.continue_suggestion_conversation(feedback)

        if result["success"]:
            print(f"ğŸ¤– AI: {result['ai_response']}")
            print()
        else:
            print(f"âŒ å¯¹è¯å¤±è´¥: {result['error']}")
            break

    # è·å–æœ€ç»ˆå»ºè®®
    final_suggestion = llm_client.get_final_suggestion()
    if final_suggestion:
        print("âœ… æœ€ç»ˆå»ºè®®:")
        print(json.dumps(final_suggestion, ensure_ascii=False, indent=2))
        print()

        # è®°å½•å»ºè®®å†å²
        conversation_history = getattr(llm_client, "conversation_history", [])
        file_operator.record_suggestion_history(
            original_suggestion=mock_suggestion,
            final_suggestion=final_suggestion,
            conversation_history=conversation_history,
            user_decision="accepted",
        )

        print("ğŸ“ å»ºè®®å†å²å·²è®°å½•")

        # æ˜¾ç¤ºå†å²è®°å½•æ‘˜è¦
        history = file_operator.get_suggestion_history()
        if history:
            latest = history[-1]
            print(f"ğŸ“Š å†å²è®°å½•æ‘˜è¦:")
            print(f"   - å»ºè®®ID: {latest['suggestion_id']}")
            print(f"   - ç”¨æˆ·å†³å®š: {latest['user_decision']}")
            print(f"   - å¯¹è¯è½®æ•°: {len(latest.get('conversation_history', []))}")
            print(f"   - æ—¶é—´æˆ³: {latest['timestamp']}")


def test_suggestion_refinement():
    """æµ‹è¯•å»ºè®®å®Œå–„åŠŸèƒ½"""
    print("\n" + "=" * 50)
    print("ğŸ§ª æµ‹è¯•å»ºè®®å®Œå–„åŠŸèƒ½\n")

    config = Config("config.yaml")
    llm_client = LLMClient(config)

    original_suggestion = {
        "type": "create",
        "priority": "medium",
        "description": "åœ¨Areasä¸‹æ–°å¢'èŒä¸šæˆé•¿'è´£ä»»åŒº",
        "current_path": "2. Areas",
        "suggested_path": "2. Areas/èŒä¸šæˆé•¿",
        "reasoning": "éœ€è¦è¦†ç›–æ›´å¤šäººç”Ÿç»´åº¦",
    }

    user_feedback = (
        "æˆ‘è§‰å¾—'èŒä¸šæˆé•¿'å¤ªå®½æ³›äº†ï¼Œèƒ½ä¸èƒ½æ›´å…·ä½“ä¸€äº›ï¼Ÿæ¯”å¦‚åˆ†æˆæŠ€æœ¯èƒ½åŠ›å’Œè½¯æŠ€èƒ½ï¼Ÿ"
    )

    print("ğŸ“‹ åŸå§‹å»ºè®®:")
    print(json.dumps(original_suggestion, ensure_ascii=False, indent=2))
    print(f"\nğŸ’¬ ç”¨æˆ·åé¦ˆ: {user_feedback}")

    result = llm_client.refine_suggestion_interactive(
        original_suggestion=original_suggestion, user_feedback=user_feedback
    )

    if result["success"]:
        print("\nâœ… å®Œå–„åçš„å»ºè®®:")
        print(json.dumps(result["refined_suggestion"], ensure_ascii=False, indent=2))
    else:
        print(f"\nâŒ å»ºè®®å®Œå–„å¤±è´¥: {result['error']}")


if __name__ == "__main__":
    print("ğŸš€ Note PARA Sweep - äº¤äº’å¼å»ºè®®è®¨è®ºåŠŸèƒ½æµ‹è¯•\n")

    try:
        test_interactive_discussion()
        test_suggestion_refinement()

        print("\n" + "=" * 50)
        print("âœ… æ‰€æœ‰æµ‹è¯•å®Œæˆï¼")
        print("\nğŸ’¡ ä½¿ç”¨æç¤º:")
        print(
            "   è¿è¡Œ 'poetry run note-para-sweep optimize' ç„¶åé€‰æ‹© 'd' æ¥ä½“éªŒäº¤äº’å¼è®¨è®ºåŠŸèƒ½"
        )

    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback

        traceback.print_exc()
